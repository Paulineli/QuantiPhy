<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/QuantiPhy_logo_pure.png" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/switch_videos.js"></script>
  <!-- MathJax for mathematical formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<!-- Navigation bar commented out
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
-->


<!-- QuantiPhy Title and Authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="quantiphy">QuantiPhy:</span> A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Li Puyin<sup>üå≤*</sup>,</span>
            <span class="author-block">
              Tiange Xiang<sup>üå≤*</sup>,</span>
            <span class="author-block">
              Ella Mao<sup>üå≤*</sup>,
            </span>
            <br>
            <span class="author-block">
              Shirley Wei<sup>üå≤</sup>,
            </span>
            <span class="author-block">
              Xinye Chen<sup>üå≤</sup>,
            </span>
            <span class="author-block">
              Adnan Masood<sup>üåç</sup>,
            </span>
            <br>
            <span class="author-block">
              Li Fei-fei<sup>üå≤‚Ä†</sup>,
            </span>
            <span class="author-block">
              Ehsan Adeli<sup>üå≤‚Ä†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>üå≤</sup>Stanford University,</span>
            <span class="author-block"><sup>üåç</sup>UST Global</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span>* Equal first authorship</span>
            <span style="margin-left: 1.5rem;">‚Ä† Equal last authorship</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Old teaser commented out
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
-->

<!-- Interactive Video Q&A Section -->
<section class="section" style="padding-top: 1rem; padding-bottom: 1rem; background-color: rgb(251, 239, 222);">
  <div class="container" style="max-width: 1100px;">
    <p style="text-align: center; font-weight: bold; margin-top: 10px; margin-bottom: 10px; font-size: 2em;">
      <!-- Compare your physical reasoning abilities with ChatGPT-5.1! -->
      Are you better than ChatGPT-5.1 at physical reasoning?
    </p>

    <div class="l-body">
      <!-- Preview Images in a Flex Container -->
      <div class="preview-container">
        <img id="Touristvideo2Preview" class="preview" src="./static/images/cropped_thumbnails/billiard.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video2Container', 'video2Preview')">
        <img id="Touristvideo3Preview" class="preview" src="./static/images/cropped_thumbnails/elephant.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video3Container', 'video3Preview')">
        <img id="Touristvideo4Preview" class="preview" src="./static/images/cropped_thumbnails/woman.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video4Container', 'video4Preview')">
        <img id="Touristvideo6Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_1.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video6Container', 'video6Preview')">
        <img id="Touristvideo7Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_2.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video7Container', 'video7Preview')">
        <img id="Touristvideo8Preview" class="preview" src="./static/images/cropped_thumbnails/museum.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video8Container', 'video8Preview')">
        <img id="Touristvideo9Preview" class="preview" src="./static/images/cropped_thumbnails/cabin.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video9Container', 'video9Preview')">
      </div>

      <!-- Video 1 -->
      <div id="Touristvideo2Container" class="video-container">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample3_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The diameter of the billiard balls is 57.4 mm. </p>
            <p><strong>Question:</strong> What is the velocity of the orange ball at 1.00s in cm/s?</p>
            <div id="answer-2" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 24.99</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-2', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 2 -->
      <div id="Touristvideo3Container" class="video-container" style="display:none;">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample1_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The big elephant's average walking speed is 2.31 m/s. </p>
            <p><strong>Question:</strong> What is the height of the small elephant in meters?</p>
            <div id="answer-3" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.20</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-3', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 3 -->
      <div id="Touristvideo4Container" class="video-container" style="display:none;">
        <div class="video-label">Object Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample2_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The woman's average walking speed is 1.25 m/s. </p>
            <p><strong>Question:</strong> What is the distance between the two black road signs in meters?</p>
            <div id="answer-4" style="display:none; text-align: center;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 4.77</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <div style="margin-top: 20px; text-align: center;">
              <p class="click-hint" style="width:85%; cursor:pointer;" onclick="toggleAnswer('answer-4', this)">
                <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
                <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- Video 4 -->
      <div id="Touristvideo6Container" class="video-container" style="display:none;">
        <div class="video-label">Relative Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample1_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?</p>
            <p><strong>Options:</strong></p>
            <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
              <li><input type="radio" name="q6"> A. Table</li>
              <li><input type="radio" name="q6"> B. Stool</li>
              <li><input type="radio" name="q6"> C. Sofa</li>
              <li><input type="radio" name="q6"> D. Stove</li>
            </ul>
            <div id="answer-6" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> Table</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> Sofa</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-6', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 5 -->
      <div id="Touristvideo7Container" class="video-container" style="display:none;">
        <div class="video-label">Absolute Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample2_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, what is the distance between the table and the piano (in meters)?</p>
            <div id="answer-7" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.3</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.1</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-7', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 6 -->
      <div id="Touristvideo8Container" class="video-container" style="display:none;">
        <div class="video-label">Appearance Order</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample3_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What will be the first-time appearance order of the following categories in the video: blanket, trash can, microwave, plant?</p>
            <p><strong>Options:</strong></p>
            <div style="display: flex; justify-content: center; flex-wrap: wrap; max-width: 800px; margin: 0 auto; gap: 20px;">
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionA8" name="q8"> 
                <label for="optionA8">A. microwave, blanket, plant, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionB8" name="q8">
                <label for="optionB8">B. plant, blanket, microwave, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionC8" name="q8">
                <label for="optionC8">C. plant, blanket, trash can, microwave</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionD8" name="q8">
                <label for="optionD8">D. blanket, trash can, microwave, plant</label>
              </div>
            </div>
          </div>
        </div>
        <div id="answer-8" style="display:none; text-align: center; margin-top: 20px;">
          <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
            <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> C.</p>
            <p><strong style="color: rgb(185,87,59);">Gemini:</strong> B.</p>
          </div>
        </div>
        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-8', this)">
          <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
          <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
        </p>
      </div>

      <!-- Video 7 -->
      <div id="Touristvideo9Container" class="video-container" style="display:none;">
        <div class="video-label">Room Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample4_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space.</p>
            <div id="answer-9" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 29.0</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 50</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-9', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

<script>
function toggleAnswer(answerId, element) {
  const answerElement = document.getElementById(answerId);
  const isVisible = answerElement.style.display === "block";

  if (isVisible) {
    answerElement.style.display = "none";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Click to view Ground Truth and ChatGPT-5.1\'s answer!</strong>';
  } else {
    answerElement.style.display = "block";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Hide answer</strong>';
  }
}
</script>
<!-- End Interactive Video Q&A Section -->
<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3" style="color: rgb(0,0,0);">Introduction</h2>
        <div class="content has-text-justified" style="color: rgb(0,0,0);">
          <p>
            Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. 
            <br>
            <br>
            To address this, we present <span class="quantiphy">QuantiPhy</span>, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video‚Äìtext instances with numerical ground truth, <span class="quantiphy">QuantiPhy</span> evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. 
            <br>
            <br>
            We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. <span class="quantiphy">QuantiPhy</span> offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    
<!-- QuantiPhy Teaser Image -->
<section class="hero teaser" style="padding-top: 0;">
  <div class="hero-body" style="padding: 0;">
    <img src="./static/images/teaser_try2.png" alt="QuantiPhy Teaser" style="width: 100%; display: block;">
  </div>
</section>

<div class="container is-max-desktop" style="padding: 1.5rem; padding-bottom: 0.5rem;">
  <p class="has-text-justified" style="font-size: 0.95rem; margin-bottom: 0;">
    <strong>Figure 1.</strong> On a crowded city street, a bird's nest falls from a branch, a car rushes by, an eagle flits over a building, and a person walks in a crosswalk ‚Äî the real world is full of complex physical motion. To enable AI to understand and navigate this environment, it is essential for generalist embodied systems to reason about physical properties quantitatively. Because objects obey common laws of physics, their kinematic properties (such as size, velocity, and acceleration) are interrelated. This interdependence makes it possible for visual AI to systematically reason about these properties with respect to available priors. In this work, we present <span class="quantiphy" style="font-family: 'Times New Roman', Times, serif; font-weight: bold;">Quantiphy</span>, the first benchmark to evaluate the reasoning ability of AI models on quantitative kinematic inference tasks.
  </p>
</div>
  </div>
</section>

<!-- QuantiPhy Dataset Section -->
<section style="padding-top: 1rem; padding-bottom: 2rem; background-color: rgb(251, 239, 222); width: 100%;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">QuantiPhy Dataset</h2>
        </div>
      </div>
  <!-- Dataset Overview Subsection -->
  <div class="container is-max-desktop" style="margin-top: 1rem;">
    
    <!-- Image Slider -->
    <div class="slider-wrapper" style="margin: 1rem 0; background-color: white; padding: 1rem; border-radius: 8px;">
      <div class="slider-container" style="position: relative; overflow: hidden;">
        <div id="dataset-slider" style="display: flex; transition: transform 0.3s ease; align-items: center;">
          <div class="slide" style="min-width: 100%; box-sizing: border-box; display: flex; flex-direction: column; align-items: center; justify-content: center;">
            <img src="./static/images/data_cat.png" alt="QuantiPhy Dataset Examples" style="max-width: 90%; display: block;">
            <p class="has-text-centered" style="font-size: 0.9rem; margin-top: 0.5rem; color: rgb(0,0,0);">
              <strong>Figure 2.</strong> Sample examples from <span class="quantiphy">QuantiPhy</span>, illustrating the four core task combinations.
            </p>
    </div>
          <div class="slide" style="min-width: 100%; box-sizing: border-box; display: flex; flex-direction: column; align-items: center; justify-content: center;">
            <img src="./static/images/data_overview.png" alt="QuantiPhy Data Overview" style="max-width: 75%; display: block;">
            <p class="has-text-centered" style="font-size: 0.9rem; margin-top: 0.5rem; color: rgb(0,0,0);">
              <strong>Figure 3.</strong> Dataset statistics overview.
            </p>
  </div>
        </div>
        <!-- Navigation Arrows -->
        <button onclick="slideDataset(-1)" style="position: absolute; left: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 10px 15px; cursor: pointer; border-radius: 5px; font-size: 1.2rem;">‚ùÆ</button>
        <button onclick="slideDataset(1)" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 10px 15px; cursor: pointer; border-radius: 5px; font-size: 1.2rem;">‚ùØ</button>
        </div>
      <!-- Dots - outside overflow hidden -->
      <div style="text-align: center; margin-top: 1rem;">
        <span id="dot-0" onclick="goToSlide(0)" style="height: 12px; width: 12px; margin: 0 4px; background-color: rgb(11,61,62); border-radius: 50%; display: inline-block; cursor: pointer;"></span>
        <span id="dot-1" onclick="goToSlide(1)" style="height: 12px; width: 12px; margin: 0 4px; background-color: #bbb; border-radius: 50%; display: inline-block; cursor: pointer;"></span>
      </div>
    </div>
    
    <h1 class="title is-4" style="color: rgb(0,0,0); margin-top: 1.5rem;">Dataset Overview</h1>
    
    <script>
      let currentSlide = 0;
      const totalSlides = 2;
      
      function slideDataset(direction) {
        currentSlide += direction;
        if (currentSlide < 0) currentSlide = totalSlides - 1;
        if (currentSlide >= totalSlides) currentSlide = 0;
        updateSlider();
      }
      
      function goToSlide(index) {
        currentSlide = index;
        updateSlider();
      }
      
      function updateSlider() {
        document.getElementById('dataset-slider').style.transform = `translateX(-${currentSlide * 100}%)`;
        for (let i = 0; i < totalSlides; i++) {
          document.getElementById(`dot-${i}`).style.backgroundColor = i === currentSlide ? 'rgb(11,61,62)' : '#bbb';
        }
      }
    </script>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> introduces a rigorous benchmark for evaluating <strong>quantitative physical reasoning</strong> in Vision-Language Models. Unlike traditional VQA tasks that focus on qualitative descriptions, QuantiPhy challenges models to perform precise numerical inference grounded in physical laws.
      </p>
      
      <ul style="list-style-type: none; padding-left: 0; margin-top: 0.75rem;">
        <li style="margin-bottom: 0.4rem;">
          <strong>üÜï Novel Task: Kinematic Inference</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            We formally define a task where object size, velocity, and acceleration are treated as mutually constraining quantities.
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Input:</strong> A video clip + A <strong>single physical prior</strong> provided as text (e.g., "The car is 4 meters long" or "Gravity is 9.8 m/s¬≤").</li>
            <li><strong>Reasoning:</strong> The model must use the provided prior to recover the <strong>world-to-pixel scale</strong> and leverage kinematic equations to deduce other unknown properties of the target object.</li>
            <li><strong>Output:</strong> A precise <strong>numerical value</strong> (with units) for a target property (e.g., "The velocity at t=2s is 12.5 m/s").</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.4rem;">
          <strong>üóÇÔ∏è Structured Taxonomy</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            To provide fine-grained analysis of model capabilities, the benchmark is organized along two primary axes:
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Dimensionality:</strong> <strong>2D</strong> (Planar motion) vs. <strong>3D</strong> (Depth-varying motion).</li>
            <li><strong>Physical Prior:</strong> <strong>Static</strong> (Size-based) vs. <strong>Dynamic</strong> (Motion-based, e.g., Velocity/Acceleration).</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.25rem;">
          <strong>üìä Scale & Diversity</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem;">
            The dataset contains <strong>3,355</strong> video-question pairs derived from <strong>569</strong> unique videos. The data spans diverse sources (Simulation, Lab, Internet) to ensure coverage across microscopic, macroscopic, and astronomical scales.
          </p>
        </li>
      </ul>
        </div>
          </div>
  
  <!-- Dataset Construction Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2rem;">
    <img src="./static/images/data_construction.png" alt="Dataset Construction Pipeline" style="max-width: 100%; display: block; margin: 0 auto 1.5rem auto; border-radius: 12px;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Dataset Construction</h1>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> is built from four complementary data sources, each designed to probe quantitative physical reasoning under different visual conditions. For every video, we segment object trajectories and annotate size, velocity, and acceleration using source-appropriate, physically grounded procedures.
      </p>
      
      <ul style="list-style-type: none; padding-left: 0; margin-top: 0.75rem;">
        <li style="margin-bottom: 0.4rem;">
          <strong>üîß Simulation (Fully Controlled)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Physically grounded Blender simulations with explicitly controlled object properties and motions</li>
            <li><strong>Segmentation:</strong> Automatic object-level segmentation from the simulation engine</li>
            <li><strong>Annotation:</strong> Exact ground-truth physical quantities directly obtained from simulation parameters</li>
            <li><strong>Key advantage:</strong> Enables precise causal and counterfactual interventions.</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.4rem;">
          <strong>üß™ Laboratory (Measured Reality)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Multi-camera recordings of real objects in motion under calibrated setups</li>
            <li><strong>Segmentation:</strong> Frame-level object segmentation via semi-automatic pipelines with manual verification</li>
            <li><strong>Annotation:</strong> Physical quantities computed from measured trajectories and known object dimensions</li>
            <li><strong>Key advantage:</strong> Provides real-world grounding with reliable physical measurements.</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.25rem;">
          <strong>üåç Internet (In-the-Wild)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Curated real-world videos with visible, trackable object motion</li>
            <li><strong>Segmentation:</strong> Manual and model-assisted object segmentation under diverse visual conditions</li>
            <li><strong>Annotation:</strong> Expert annotations constrained by visual evidence and physical plausibility</li>
            <li><strong>Key advantage:</strong> Tests robustness under natural noise, clutter, and ambiguity.</li>
          </ul>
        </li>
      </ul>
        </div>
        </div>
    
    <!-- Figure with caption -->
    <!-- <figure style="margin: 1.5rem 0;">
      <img src="./static/images/YOUR_IMAGE.png" alt="Description" style="max-width: 100%; display: block; margin: 0 auto;">
      <figcaption style="text-align: center; margin-top: 1rem; color: rgb(0,0,0);">
        <strong>Figure X: Title.</strong>  -->
        <!-- Add your figure caption here -->
      <!-- </figcaption>
    </figure> -->
      </div>
</section>
<!-- End QuantiPhy Dataset Section -->

<!-- Evaluation on QuantiPhy Section -->
<section class="section" style="background-color: white;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">Evaluation on QuantiPhy</h2>
    </div>
  </div>
  
  <!-- Evaluation Setup Subsection -->
  <div class="container is-max-desktop" style="margin-top: 1rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Evaluation Setup</h1>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> evaluates whether vision‚Äìlanguage models can infer quantitative physical properties from visual evidence, rather than producing plausible numerical guesses. Each evaluation instance consists of a short video paired with a natural-language question, and models are required to output a single continuous numerical value, such as object size, velocity, or acceleration. All tasks are open-ended and avoid multiple-choice formats, preventing shortcut strategies based on memorization or priors.
      </p>
      <p class="has-text-justified">
        We evaluate three core tasks‚Äîsize, velocity, and acceleration estimation‚Äîusing a unified protocol across simulated, laboratory, and in-the-wild videos. Human performance is measured under the same setup, providing a reference point for visually grounded quantitative reasoning.
      </p>
      <p class="has-text-justified">
        To measure performance, we use <strong>Mean Relative Accuracy (MRA)</strong>, a metric designed for quantitative physical inference. For a prediction \(\hat{y}\) and ground-truth value \(y\), we compute the relative error \(|\hat{y} - y| / |y|\). A prediction is considered correct under a confidence threshold \(\theta\) if
      </p>
      <p style="text-align: center; margin: 1rem 0;">
        \[\frac{|\hat{y} - y|}{|y|} < 1 - \theta,\]
      </p>
      <p class="has-text-justified">
        where \(\theta \in \{0.5, 0.55, \ldots, 0.95\}\). The final MRA score is the average accuracy across all thresholds:
      </p>
      <p style="text-align: center; margin: 1rem 0;">
        \[\text{MRA} = \frac{1}{|C|} \sum_{\theta \in C} \mathbb{1}\left(\frac{|\hat{y} - y|}{|y|} < 1 - \theta\right),\]
      </p>
      <p class="has-text-justified">
        with \(C\) denoting the set of confidence thresholds.
      </p>
      <p class="has-text-justified">
        By rewarding approximate correctness rather than exact numerical matches, MRA provides a smoother and more informative measure of quantitative physical reasoning under real-world visual uncertainty.
          </p>
        </div>
    
    <!-- Explain Box -->
    <div style="background: linear-gradient(135deg, rgba(11,61,62,0.08) 0%, rgba(229,76,45,0.08) 100%); border-left: 4px solid rgb(11,61,62); border-radius: 8px; padding: 1.5rem; margin-top: 1.5rem;">
      <h3 style="color: rgb(11,61,62); margin-bottom: 1rem; font-size: 1.1rem;">
        üß† <strong>Explain Box: Why Mean Relative Accuracy (MRA)?</strong>
      </h3>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
        <strong>Exact numerical matching is unrealistic for physical reasoning.</strong> Small perceptual differences in video can lead to meaningful but imperfect numerical estimates, even for humans.
      </p>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 0.75rem;">
        <strong>Mean Relative Accuracy (MRA)</strong> addresses this by:
      </p>
      <ul style="margin-left: 1.5rem; color: rgb(0,0,0);">
        <li>Measuring <strong>relative error</strong> rather than absolute error</li>
        <li>Assigning <strong>partial credit</strong> for approximately correct predictions</li>
        <li>Avoiding <strong>brittle pass/fail judgments</strong> on noisy visual data</li>
      </ul>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-top: 1rem; margin-bottom: 0;">
        As a result, MRA better reflects how physical quantities are inferred in practice and provides a smoother, more informative signal for comparing models.
      </p>
      </div>
    </div>
  
  <!-- QuantiPhy Leaderboard Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2.5rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">QuantiPhy Leaderboard <img src="./static/images/QuantiPhy_logo.png" alt="" style="height: 1.2em; vertical-align: middle; margin-left: 0.3rem;"></h1>
    
    <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
      To include your model in the leaderboard, please email <a href="mailto:puyinli@stanford.edu" style="color: rgb(229,76,45);">puyinli@stanford.edu</a> with evaluation logs and setups.
    </p>
    
    <style>
      .quantiphy-leaderboard {
        width: 100%;
        border-collapse: separate;
        border-spacing: 0;
        background: #ffffff;
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        font-size: 14px;
        margin-bottom: 1rem;
      }
      .quantiphy-leaderboard thead {
        background: rgb(11,61,62);
        color: white;
      }
      .quantiphy-leaderboard th {
        padding: 12px 10px;
        text-align: center;
        font-weight: 600;
        font-size: 13px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        color: white;
      }
      .quantiphy-leaderboard td {
        padding: 10px 8px;
        border-bottom: 1px solid #e2e8f0;
        color: #4a5568;
        text-align: center;
      }
      .quantiphy-leaderboard tbody tr:hover {
        background-color: #f7fafc;
      }
      .quantiphy-leaderboard td:first-child {
        font-weight: 600;
        text-align: right;
        color: #2d3748;
        padding-right: 15px;
      }
      .quantiphy-leaderboard .section-header {
        background-color: rgba(11,61,62,0.1);
        font-weight: 700;
        color: #2d3748;
        font-size: 14px;
        text-align: left;
        padding: 10px 15px !important;
      }
      .quantiphy-leaderboard .section-header td {
        text-align: left;
        font-weight: 700;
        font-style: italic;
      }
      .quantiphy-leaderboard .human-row {
        background: rgba(229,76,45,0.08) !important;
        font-weight: 600;
      }
      .quantiphy-leaderboard .human-row td {
        border-top: 2px solid rgb(229,76,45);
        border-bottom: 2px solid rgb(229,76,45);
      }
      .quantiphy-leaderboard .best-score {
        font-weight: 700;
        background-color: rgba(229,76,45,0.15);
      }
      .quantiphy-leaderboard .best-open {
        font-weight: 700;
        background-color: rgba(11,61,62,0.12);
      }
      .quantiphy-leaderboard tbody tr:nth-child(even):not(.section-header):not(.human-row) {
        background-color: #fafafa;
      }
    </style>
    
    <div style="overflow-x: auto;">
      <table class="quantiphy-leaderboard">
        <thead>
          <tr>
            <th style="text-align: right;">Models</th>
            <th>Size</th>
            <th>Release</th>
            <th colspan="4" style="border-bottom: 2px solid rgba(255,255,255,0.3);">Kinematic Categories</th>
            <th>Avg.</th>
          </tr>
          <tr>
            <th></th>
            <th></th>
            <th></th>
            <th>2S</th>
            <th>2D</th>
            <th>3S</th>
            <th>3D</th>
            <th></th>
          </tr>
        </thead>
        <tbody>
          <!-- Proprietary Models -->
          <tr class="section-header">
            <td colspan="8">Proprietary models</td>
          </tr>
          <tr>
            <td>ChatGPT-5.1</td>
            <td>‚Äì</td>
            <td>2025/11</td>
            <td class="best-score">46.1</td>
            <td>56.1</td>
            <td class="best-score">45.0</td>
            <td class="best-score">58.1</td>
            <td class="best-score">52.6</td>
          </tr>
          <tr>
            <td>Gemini-2.5 Pro</td>
            <td>‚Äì</td>
            <td>2025/03</td>
            <td>44.3</td>
            <td class="best-score">57.3</td>
            <td>41.8</td>
            <td>53.6</td>
            <td>49.3</td>
          </tr>
          <tr>
            <td>Gemini-2.5 Flash</td>
            <td>‚Äì</td>
            <td>2025/03</td>
            <td>39.9</td>
            <td>53.0</td>
            <td>40.6</td>
            <td>56.2</td>
            <td>47.4</td>
          </tr>
          <tr>
            <td>ChatGPT-5</td>
            <td>‚Äì</td>
            <td>2025/04</td>
            <td>36.4</td>
            <td>34.9</td>
            <td>24.7</td>
            <td>32.9</td>
            <td>32.2</td>
          </tr>
          <tr>
            <td>Claude-4.5 Sonnet</td>
            <td>‚Äì</td>
            <td>2025/04</td>
            <td>19.6</td>
            <td>22.9</td>
            <td>19.6</td>
            <td>29.1</td>
            <td>22.8</td>
          </tr>
          
          <!-- Open-weight Models -->
          <tr class="section-header">
            <td colspan="8">Open-weight models</td>
          </tr>
          <tr>
            <td>InternVL-3.5-30B</td>
            <td>30B</td>
            <td>2025/05</td>
            <td class="best-open">36.7</td>
            <td>45.4</td>
            <td class="best-open">38.0</td>
            <td class="best-open">41.6</td>
            <td class="best-open">40.4</td>
          </tr>
          <tr>
            <td>Qwen3-VL-Instruct-8B</td>
            <td>8B</td>
            <td>2025/04</td>
            <td>26.0</td>
            <td class="best-open">47.8</td>
            <td>33.5</td>
            <td>46.0</td>
            <td>38.3</td>
          </tr>
          <tr>
            <td>Molmo-7B</td>
            <td>7B</td>
            <td>2024/09</td>
            <td>30.0</td>
            <td>43.1</td>
            <td>24.4</td>
            <td>36.6</td>
            <td>33.5</td>
          </tr>
          <tr>
            <td>Phi-4-Multimodal-Instruct</td>
            <td>5.6B</td>
            <td>2025/02</td>
            <td>33.3</td>
            <td>42.3</td>
            <td>23.2</td>
            <td>27.9</td>
            <td>31.7</td>
          </tr>
          <tr>
            <td>SmolVLM-Instruct</td>
            <td>0.26B</td>
            <td>2024/11</td>
            <td>31.6</td>
            <td>34.4</td>
            <td>19.8</td>
            <td>27.8</td>
            <td>28.4</td>
          </tr>
          <tr>
            <td>VILA-7B</td>
            <td>7B</td>
            <td>2024/01</td>
            <td>23.0</td>
            <td>29.8</td>
            <td>14.4</td>
            <td>23.0</td>
            <td>22.6</td>
          </tr>
          <tr>
            <td>CogVLM2 Video</td>
            <td>12B</td>
            <td>2024/05</td>
            <td>19.4</td>
            <td>28.7</td>
            <td>12.7</td>
            <td>27.9</td>
            <td>22.2</td>
          </tr>
          <tr>
            <td>Phi-3-Mini-128K-Instruct-3.8B</td>
            <td>3.8B</td>
            <td>2024/04</td>
            <td>17.3</td>
            <td>14.7</td>
            <td>19.2</td>
            <td>18.5</td>
            <td>17.4</td>
          </tr>
          <tr>
            <td>LLaVA-13B</td>
            <td>13B</td>
            <td>2023/10</td>
            <td>14.4</td>
            <td>22.1</td>
            <td>8.0</td>
            <td>16.5</td>
            <td>15.2</td>
          </tr>
          <tr>
            <td>MiniCPM-V 4.5-8B</td>
            <td>8B</td>
            <td>2025/05</td>
            <td>27.6</td>
            <td>26.1</td>
            <td>0.4</td>
            <td>0.0</td>
            <td>13.5</td>
          </tr>
          <tr>
            <td>Fuyu-8B</td>
            <td>8B</td>
            <td>2023/10</td>
            <td>9.5</td>
            <td>14.7</td>
            <td>9.5</td>
            <td>16.2</td>
            <td>12.5</td>
          </tr>
          
          <!-- Human Baseline -->
          <tr class="human-row">
            <td>Human Baseline</td>
            <td>‚Äì</td>
            <td>‚Äì</td>
            <td>50.0</td>
            <td>59.1</td>
            <td>55.2</td>
            <td>57.9</td>
            <td>55.6</td>
          </tr>
        </tbody>
      </table>
    </div>
    
    <p style="font-size: 0.85rem; color: #666; margin-top: 0.5rem;">
      <strong>Note:</strong> Best proprietary model scores are highlighted in <span style="background-color: rgba(229,76,45,0.15); padding: 2px 6px; border-radius: 3px;">orange</span>, best open-weight model scores in <span style="background-color: rgba(11,61,62,0.12); padding: 2px 6px; border-radius: 3px;">teal</span>.
    </p>
  </div>
</section>
<!-- End Evaluation on QuantiPhy Section -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

