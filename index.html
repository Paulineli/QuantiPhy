<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/QuantiPhy_logo_pure.png" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/switch_videos.js"></script>
</head>
<body>

<!-- Navigation bar commented out
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>
-->


<!-- QuantiPhy Title and Authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="quantiphy">QuantiPhy:</span> A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Li Puyin<sup>1</sup>*,</span>
            <span class="author-block">
              Tiange Xiang<sup>1</sup>*,</span>
            <span class="author-block">
              Ella Mao<sup>1</sup>*,
            </span>
            <br>
            <span class="author-block">
              Shirley Wei<sup>1</sup>,
            </span>
            <span class="author-block">
              Xinye Chen<sup>1</sup>,
            </span>
            <span class="author-block">
              Adnan Masood<sup>2</sup>,
            </span>
            <span class="author-block">
              Li Fei-fei<sup>1</sup>‚Ä†,
            </span>
            <span class="author-block">
              Ehsan Adeli<sup>1</sup>‚Ä†
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University,</span>
            <span class="author-block"><sup>2</sup>UST Global</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Old teaser commented out
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
-->

<!-- Interactive Video Q&A Section -->
<section class="section" style="padding-top: 1rem; padding-bottom: 1rem; background-color: rgb(251, 239, 222);">
  <div class="container" style="max-width: 1100px;">
    <p style="text-align: center; font-weight: bold; margin-top: 10px; margin-bottom: 10px; font-size: 2em;">
      Compare your physical reasoning abilities with ChatGPT-5.1!
    </p>

    <div class="l-body">
      <!-- Preview Images in a Flex Container -->
      <div class="preview-container">
        <img id="Touristvideo2Preview" class="preview" src="./static/images/cropped_thumbnails/billiard.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video2Container', 'video2Preview')">
        <img id="Touristvideo3Preview" class="preview" src="./static/images/cropped_thumbnails/elephant.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video3Container', 'video3Preview')">
        <img id="Touristvideo4Preview" class="preview" src="./static/images/cropped_thumbnails/woman.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video4Container', 'video4Preview')">
        <img id="Touristvideo6Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_1.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video6Container', 'video6Preview')">
        <img id="Touristvideo7Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_2.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video7Container', 'video7Preview')">
        <img id="Touristvideo8Preview" class="preview" src="./static/images/cropped_thumbnails/museum.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video8Container', 'video8Preview')">
        <img id="Touristvideo9Preview" class="preview" src="./static/images/cropped_thumbnails/cabin.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video9Container', 'video9Preview')">
      </div>

      <!-- Video 1 -->
      <div id="Touristvideo2Container" class="video-container">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample3_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The diameter of the billiard balls is 57.4 mm. </p>
            <p><strong>Question:</strong> What is the velocity of the orange ball at 1.00s in cm/s?</p>
            <div id="answer-2" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 24.99</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-2', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 2 -->
      <div id="Touristvideo3Container" class="video-container" style="display:none;">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample1_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The big elephant's average walking speed is 2.31 m/s. </p>
            <p><strong>Question:</strong> What is the height of the small elephant in meters?</p>
            <div id="answer-3" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.20</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-3', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 3 -->
      <div id="Touristvideo4Container" class="video-container" style="display:none;">
        <div class="video-label">Object Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample2_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The woman's average walking speed is 1.25 m/s. </p>
            <p><strong>Question:</strong> What is the distance between the two black road signs in meters?</p>
            <div id="answer-4" style="display:none; text-align: center;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 4.77</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <div style="margin-top: 20px; text-align: center;">
              <p class="click-hint" style="width:85%; cursor:pointer;" onclick="toggleAnswer('answer-4', this)">
                <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
                <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- Video 4 -->
      <div id="Touristvideo6Container" class="video-container" style="display:none;">
        <div class="video-label">Relative Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample1_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?</p>
            <p><strong>Options:</strong></p>
            <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
              <li><input type="radio" name="q6"> A. Table</li>
              <li><input type="radio" name="q6"> B. Stool</li>
              <li><input type="radio" name="q6"> C. Sofa</li>
              <li><input type="radio" name="q6"> D. Stove</li>
            </ul>
            <div id="answer-6" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> Table</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> Sofa</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-6', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 5 -->
      <div id="Touristvideo7Container" class="video-container" style="display:none;">
        <div class="video-label">Absolute Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample2_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, what is the distance between the table and the piano (in meters)?</p>
            <div id="answer-7" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.3</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.1</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-7', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 6 -->
      <div id="Touristvideo8Container" class="video-container" style="display:none;">
        <div class="video-label">Appearance Order</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample3_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What will be the first-time appearance order of the following categories in the video: blanket, trash can, microwave, plant?</p>
            <p><strong>Options:</strong></p>
            <div style="display: flex; justify-content: center; flex-wrap: wrap; max-width: 800px; margin: 0 auto; gap: 20px;">
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionA8" name="q8"> 
                <label for="optionA8">A. microwave, blanket, plant, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionB8" name="q8">
                <label for="optionB8">B. plant, blanket, microwave, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionC8" name="q8">
                <label for="optionC8">C. plant, blanket, trash can, microwave</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionD8" name="q8">
                <label for="optionD8">D. blanket, trash can, microwave, plant</label>
              </div>
            </div>
          </div>
        </div>
        <div id="answer-8" style="display:none; text-align: center; margin-top: 20px;">
          <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
            <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> C.</p>
            <p><strong style="color: rgb(185,87,59);">Gemini:</strong> B.</p>
          </div>
        </div>
        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-8', this)">
          <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
          <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
        </p>
      </div>

      <!-- Video 7 -->
      <div id="Touristvideo9Container" class="video-container" style="display:none;">
        <div class="video-label">Room Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample4_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space.</p>
            <div id="answer-9" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 29.0</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 50</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-9', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

<script>
function toggleAnswer(answerId, element) {
  const answerElement = document.getElementById(answerId);
  const isVisible = answerElement.style.display === "block";

  if (isVisible) {
    answerElement.style.display = "none";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Click to view Ground Truth and ChatGPT-5.1\'s answer!</strong>';
  } else {
    answerElement.style.display = "block";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Hide answer</strong>';
  }
}
</script>
<!-- End Interactive Video Q&A Section -->
<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3" style="color: rgb(0,0,0);">Introduction</h2>
        <div class="content has-text-justified" style="color: rgb(0,0,0);">
          <p>
            Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. 
            <br>
            <br>
            To address this, we present <span class="quantiphy">QuantiPhy</span>, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video‚Äìtext instances with numerical ground truth, <span class="quantiphy">QuantiPhy</span> evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. 
            <br>
            <br>
            We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. <span class="quantiphy">QuantiPhy</span> offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    
<!-- QuantiPhy Teaser Image -->
<section class="hero teaser" style="padding-top: 0;">
  <div class="hero-body" style="padding: 0;">
    <img src="./static/images/teaser_try2.png" alt="QuantiPhy Teaser" style="width: 100%; display: block;">
  </div>
</section>

<div class="container is-max-desktop" style="padding: 1.5rem; padding-bottom: 0.5rem;">
  <p class="has-text-justified" style="font-size: 0.95rem; margin-bottom: 0;">
    <strong>Figure 1.</strong> On a crowded city street, a bird's nest falls from a branch, a car rushes by, an eagle flits over a building, and a person walks in a crosswalk ‚Äî the real world is full of complex physical motion. To enable AI to understand and navigate this environment, it is essential for generalist embodied systems to reason about physical properties quantitatively. Because objects obey common laws of physics, their kinematic properties (such as size, velocity, and acceleration) are interrelated. This interdependence makes it possible for visual AI to systematically reason about these properties with respect to available priors. In this work, we present <span class="quantiphy" style="font-family: 'Times New Roman', Times, serif; font-weight: bold;">Quantiphy</span>, the first benchmark to evaluate the reasoning ability of AI models on quantitative kinematic inference tasks.
  </p>
</div>
  </div>
</section>

<!-- QuantiPhy Dataset Section -->
<section style="padding-top: 1rem; padding-bottom: 2rem; background-color: rgb(251, 239, 222); width: 100%;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">QuantiPhy Dataset</h2>
    </div>
  </div>
  <div class="container is-max-desktop">
    <img src="./static/images/data_cat.png" alt="QuantiPhy Dataset" style="max-width: 80%; display: block; margin: 1.5rem auto 0 auto;">
  </div>
  
  <!-- Dataset Overview Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Dataset Overview</h1>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> introduces a rigorous benchmark for evaluating <strong>quantitative physical reasoning</strong> in Vision-Language Models. Unlike traditional VQA tasks that focus on qualitative descriptions, QuantiPhy challenges models to perform precise numerical inference grounded in physical laws.
      </p>
      
      <ul style="list-style-type: none; padding-left: 0; margin-top: 0.75rem;">
        <li style="margin-bottom: 0.4rem;">
          <strong>üÜï Novel Task: Kinematic Inference</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            We formally define a task where object size, velocity, and acceleration are treated as mutually constraining quantities.
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Input:</strong> A video clip + A <strong>single physical prior</strong> provided as text (e.g., "The car is 4 meters long" or "Gravity is 9.8 m/s¬≤").</li>
            <li><strong>Reasoning:</strong> The model must use the provided prior to recover the <strong>world-to-pixel scale</strong> and leverage kinematic equations to deduce other unknown properties of the target object.</li>
            <li><strong>Output:</strong> A precise <strong>numerical value</strong> (with units) for a target property (e.g., "The velocity at t=2s is 12.5 m/s").</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.4rem;">
          <strong>üóÇÔ∏è Structured Taxonomy</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            To provide fine-grained analysis of model capabilities, the benchmark is organized along two primary axes:
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Dimensionality:</strong> <strong>2D</strong> (Planar motion) vs. <strong>3D</strong> (Depth-varying motion).</li>
            <li><strong>Physical Prior:</strong> <strong>Static</strong> (Size-based) vs. <strong>Dynamic</strong> (Motion-based, e.g., Velocity/Acceleration).</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.25rem;">
          <strong>üìä Scale & Diversity</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem;">
            The dataset contains <strong>3,355</strong> video-question pairs derived from <strong>569</strong> unique videos. The data spans diverse sources (Simulation, Lab, Internet) to ensure coverage across microscopic, macroscopic, and astronomical scales.
          </p>
        </li>
      </ul>
    </div>
  </div>
    
    <!-- Figure with caption -->
    <!-- <figure style="margin: 1.5rem 0;">
      <img src="./static/images/YOUR_IMAGE.png" alt="Description" style="max-width: 100%; display: block; margin: 0 auto;">
      <figcaption style="text-align: center; margin-top: 1rem; color: rgb(0,0,0);">
        <strong>Figure X: Title.</strong>  -->
        <!-- Add your figure caption here -->
      <!-- </figcaption>
    </figure> -->
  </div>
</section>
<!-- End QuantiPhy Dataset Section -->

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
