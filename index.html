<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/QuantiPhy_logo_pure.png" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/switch_videos.js"></script>
  <!-- MathJax for mathematical formulas -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<!-- QuantiPhy Side Navigation -->
<nav id="quantiphy-sidebar" style="position: fixed; right: 20px; top: 50%; transform: translateY(-50%); z-index: 1000; opacity: 0; transition: opacity 0.3s ease; pointer-events: none;">
  <div style="background: white; border-radius: 16px; padding: 1rem 0.8rem; box-shadow: 0 4px 20px rgba(0,0,0,0.12); border: 1px solid rgba(11,61,62,0.1);">
    <!-- Logo -->
    <a href="#" style="display: flex; justify-content: center; margin-bottom: 1rem; text-decoration: none;">
      <img src="./static/images/QuantiPhy_logo_pure.png" alt="Logo" style="height: 2rem;">
    </a>
    
    <!-- Divider -->
    <div style="height: 1px; background: linear-gradient(to right, transparent, rgba(11,61,62,0.2), transparent); margin-bottom: 0.8rem;"></div>
    
    <!-- Nav Links -->
    <div style="display: flex; flex-direction: column; gap: 0.4rem;">
      <a href="#interactive-section" class="sidebar-link" title="Interactive Demo" style="display: flex; align-items: center; justify-content: center; width: 40px; height: 40px; border-radius: 10px; color: rgb(11,61,62); text-decoration: none; transition: all 0.2s ease; background: transparent;">
        <i class="fas fa-play-circle" style="font-size: 1.1rem;"></i>
      </a>
      <a href="#dataset-section" class="sidebar-link" title="Dataset" style="display: flex; align-items: center; justify-content: center; width: 40px; height: 40px; border-radius: 10px; color: rgb(11,61,62); text-decoration: none; transition: all 0.2s ease; background: transparent;">
        <i class="fas fa-database" style="font-size: 1.1rem;"></i>
      </a>
      <a href="#evaluation-section" class="sidebar-link" title="Evaluation" style="display: flex; align-items: center; justify-content: center; width: 40px; height: 40px; border-radius: 10px; color: rgb(11,61,62); text-decoration: none; transition: all 0.2s ease; background: transparent;">
        <i class="fas fa-chart-bar" style="font-size: 1.1rem;"></i>
      </a>
      <a href="#dissecting-section" class="sidebar-link" title="Analysis" style="display: flex; align-items: center; justify-content: center; width: 40px; height: 40px; border-radius: 10px; color: rgb(11,61,62); text-decoration: none; transition: all 0.2s ease; background: transparent;">
        <i class="fas fa-microscope" style="font-size: 1.1rem;"></i>
      </a>
    </div>

    <!-- Divider -->
    <div style="height: 1px; background: linear-gradient(to right, transparent, rgba(11,61,62,0.2), transparent); margin: 0.8rem 0;"></div>
    
    <!-- Back to top -->
    <a href="#" class="sidebar-link" title="Back to Top" style="display: flex; align-items: center; justify-content: center; width: 40px; height: 40px; border-radius: 10px; color: rgb(229,76,45); text-decoration: none; transition: all 0.2s ease; background: transparent;">
      <i class="fas fa-arrow-up" style="font-size: 1rem;"></i>
    </a>
  </div>
</nav>

<style>
  .sidebar-link:hover {
    background: rgba(11,61,62,0.1) !important;
    transform: scale(1.1);
  }
  .sidebar-link:hover[title="Back to Top"] {
    background: rgba(229,76,45,0.1) !important;
  }
  #quantiphy-sidebar.visible {
    opacity: 1 !important;
    pointer-events: auto !important;
  }
  @media (max-width: 1400px) {
    #quantiphy-sidebar {
      right: 10px;
    }
  }
  @media (max-width: 768px) {
    #quantiphy-sidebar {
      display: none;
    }
  }
</style>

<script>
  // Show/hide sidebar on scroll
  const sidebar = document.getElementById('quantiphy-sidebar');
  
  window.addEventListener('scroll', () => {
    if (window.scrollY > 400) {
      sidebar.classList.add('visible');
    } else {
      sidebar.classList.remove('visible');
    }
  });
</script>


<!-- QuantiPhy Title and Authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span class="quantiphy">QuantiPhy:</span> A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Li Puyin<sup>üå≤*</sup>,</span>
            <span class="author-block">
              Tiange Xiang<sup>üå≤*</sup>,</span>
            <span class="author-block">
              Ella Mao<sup>üå≤*</sup>,
            </span>
            <br>
            <span class="author-block">
              Shirley Wei<sup>üå≤</sup>,
            </span>
            <span class="author-block">
              Xinye Chen<sup>üå≤</sup>,
            </span>
            <span class="author-block">
              Adnan Masood<sup>üåç</sup>,
            </span>
            <br>
            <span class="author-block">
              Li Fei-fei<sup>üå≤‚Ä†</sup>,
            </span>
            <span class="author-block">
              Ehsan Adeli<sup>üå≤‚Ä†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>üå≤</sup>Stanford University,</span>
            <span class="author-block"><sup>üåç</sup>UST Global</span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
            <span>* Equal first authorship</span>
            <span style="margin-left: 1.5rem;">‚Ä† Equal last authorship</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Old teaser commented out
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>
-->

<!-- Interactive Video Q&A Section -->
<section id="interactive-section" class="section" style="padding-top: 1rem; padding-bottom: 1rem; background-color: rgb(251, 239, 222);">
  <div class="container" style="max-width: 1100px;">
    <p style="text-align: center; font-weight: bold; margin-top: 10px; margin-bottom: 10px; font-size: 2em;">
      <!-- Compare your physical reasoning abilities with ChatGPT-5.1! -->
      Are you better than ChatGPT-5.1 at physical reasoning?
    </p>

    <div class="l-body">
      <!-- Preview Images in a Flex Container -->
      <div class="preview-container">
        <img id="Touristvideo2Preview" class="preview" src="./static/images/cropped_thumbnails/billiard.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video2Container', 'video2Preview')">
        <img id="Touristvideo3Preview" class="preview" src="./static/images/cropped_thumbnails/elephant.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video3Container', 'video3Preview')">
        <img id="Touristvideo4Preview" class="preview" src="./static/images/cropped_thumbnails/woman.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video4Container', 'video4Preview')">
        <img id="Touristvideo6Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_1.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video6Container', 'video6Preview')">
        <img id="Touristvideo7Preview" class="preview" src="./static/images/cropped_thumbnails/pingpong_2.png" alt="video-thumbnail" onclick="switchVideo('Tourist', 'video7Container', 'video7Preview')">
        <img id="Touristvideo8Preview" class="preview" src="./static/images/cropped_thumbnails/museum.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video8Container', 'video8Preview')">
        <img id="Touristvideo9Preview" class="preview" src="./static/images/cropped_thumbnails/cabin.png" alt="video-thumbnail" onclick="switchVideo('Tourist','video9Container', 'video9Preview')">
      </div>

      <!-- Video 1 -->
      <div id="Touristvideo2Container" class="video-container">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample3_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The diameter of the billiard balls is 57.4 mm. </p>
            <p><strong>Question:</strong> What is the velocity of the orange ball at 1.00s in cm/s?</p>
            <div id="answer-2" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 24.99</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-2', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 2 -->
      <div id="Touristvideo3Container" class="video-container" style="display:none;">
        <div class="video-label">Object Count</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample1_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The big elephant's average walking speed is 2.31 m/s. </p>
            <p><strong>Question:</strong> What is the height of the small elephant in meters?</p>
            <div id="answer-3" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.20</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-3', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 3 -->
      <div id="Touristvideo4Container" class="video-container" style="display:none;">
        <div class="video-label">Object Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/2D_sample2_internet.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Prior Knowledge:</strong> The woman's average walking speed is 1.25 m/s. </p>
            <p><strong>Question:</strong> What is the distance between the two black road signs in meters?</p>
            <div id="answer-4" style="display:none; text-align: center;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 4.77</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.00</p>
              </div>
            </div>
            <div style="margin-top: 20px; text-align: center;">
              <p class="click-hint" style="width:85%; cursor:pointer;" onclick="toggleAnswer('answer-4', this)">
                <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
                <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
              </p>
            </div>
          </div>
        </div>
      </div>

      <!-- Video 4 -->
      <div id="Touristvideo6Container" class="video-container" style="display:none;">
        <div class="video-label">Relative Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample1_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, which of these objects (table, stool, sofa, stove) is the closest to the TV?</p>
            <p><strong>Options:</strong></p>
            <ul style="list-style-type: none; padding-left: 0; display: flex; justify-content: space-between; text-align: left; max-width: 60%; margin: 0 auto;">
              <li><input type="radio" name="q6"> A. Table</li>
              <li><input type="radio" name="q6"> B. Stool</li>
              <li><input type="radio" name="q6"> C. Sofa</li>
              <li><input type="radio" name="q6"> D. Stove</li>
            </ul>
            <div id="answer-6" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> Table</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> Sofa</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-6', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 5 -->
      <div id="Touristvideo7Container" class="video-container" style="display:none;">
        <div class="video-label">Absolute Distance</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample2_lab.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> Measuring from the closest point of each object, what is the distance between the table and the piano (in meters)?</p>
            <div id="answer-7" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 2.3</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 0.1</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-7', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

      <!-- Video 6 -->
      <div id="Touristvideo8Container" class="video-container" style="display:none;">
        <div class="video-label">Appearance Order</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample3_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What will be the first-time appearance order of the following categories in the video: blanket, trash can, microwave, plant?</p>
            <p><strong>Options:</strong></p>
            <div style="display: flex; justify-content: center; flex-wrap: wrap; max-width: 800px; margin: 0 auto; gap: 20px;">
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionA8" name="q8"> 
                <label for="optionA8">A. microwave, blanket, plant, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionB8" name="q8">
                <label for="optionB8">B. plant, blanket, microwave, trash can</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionC8" name="q8">
                <label for="optionC8">C. plant, blanket, trash can, microwave</label>
              </div>
              <div style="flex: 1; min-width: 300px; text-align: left;">
                <input type="radio" id="optionD8" name="q8">
                <label for="optionD8">D. blanket, trash can, microwave, plant</label>
              </div>
            </div>
          </div>
        </div>
        <div id="answer-8" style="display:none; text-align: center; margin-top: 20px;">
          <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
            <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> C.</p>
            <p><strong style="color: rgb(185,87,59);">Gemini:</strong> B.</p>
          </div>
        </div>
        <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-8', this)">
          <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
          <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
        </p>
      </div>

      <!-- Video 7 -->
      <div id="Touristvideo9Container" class="video-container" style="display:none;">
        <div class="video-label">Room Size</div>
        <video class="video-music" controls preload="metadata" playsinline>
          <source src="./static/videos/3D_sample4_simulation.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <div class="video-qa">
          <div class="qa-item">
            <p><strong>Question:</strong> What is the size of this room (in square meters)? If multiple rooms are shown, estimate the size of the combined space.</p>
            <div id="answer-9" style="display:none; text-align: center; margin-top: 20px;">
              <div style="display: flex; justify-content: center; align-items: center; gap: 20px; white-space: nowrap;">
                <p><strong style="color: rgb(97,124,90);">Ground Truth:</strong> 29.0</p>
                <p><strong style="color: rgb(185,87,59);">ChatGPT-5.1:</strong> 50</p>
              </div>
            </div>
            <p class="click-hint" style="width:85%; cursor:pointer; margin-top: 20px;" onclick="toggleAnswer('answer-9', this)">
              <img src="./static/images/icons/teaser.gif" style="width:1.5rem">
              <strong>Click to view Ground Truth and ChatGPT-5.1's answer!</strong>
            </p>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>

<script>
function toggleAnswer(answerId, element) {
  const answerElement = document.getElementById(answerId);
  const isVisible = answerElement.style.display === "block";

  if (isVisible) {
    answerElement.style.display = "none";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Click to view Ground Truth and ChatGPT-5.1\'s answer!</strong>';
  } else {
    answerElement.style.display = "block";
    element.innerHTML = '<img src="./static/images/icons/teaser.gif" style="width:1.5rem"> <strong>Hide answer</strong>';
  }
}
</script>
<!-- End Interactive Video Q&A Section -->
<!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3" style="color: rgb(0,0,0);">Introduction</h2>
        <div class="content has-text-justified" style="color: rgb(0,0,0);">
          <p>
            Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. 
            <br>
            <br>
            To address this, we present <span class="quantiphy">QuantiPhy</span>, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video‚Äìtext instances with numerical ground truth, <span class="quantiphy">QuantiPhy</span> evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. 
            <br>
            <br>
            We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. <span class="quantiphy">QuantiPhy</span> offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    
<!-- QuantiPhy Teaser Image -->
<section class="hero teaser" style="padding-top: 0;">
  <div class="hero-body" style="padding: 0;">
    <img src="./static/images/teaser_try2.png" alt="QuantiPhy Teaser" style="width: 100%; display: block;">
  </div>
</section>

<div class="container is-max-desktop" style="padding: 1.5rem; padding-bottom: 0.5rem;">
  <p class="has-text-justified" style="font-size: 0.95rem; margin-bottom: 0;">
    <strong>Figure 1.</strong> On a crowded city street, a bird's nest falls from a branch, a car rushes by, an eagle flits over a building, and a person walks in a crosswalk ‚Äî the real world is full of complex physical motion. To enable AI to understand and navigate this environment, it is essential for generalist embodied systems to reason about physical properties quantitatively. Because objects obey common laws of physics, their kinematic properties (such as size, velocity, and acceleration) are interrelated. This interdependence makes it possible for visual AI to systematically reason about these properties with respect to available priors. In this work, we present <span class="quantiphy" style="font-family: 'Times New Roman', Times, serif; font-weight: bold;">Quantiphy</span>, the first benchmark to evaluate the reasoning ability of AI models on quantitative kinematic inference tasks.
  </p>
</div>
  </div>
</section>

<!-- QuantiPhy Dataset Section -->
<section id="dataset-section" style="padding-top: 1rem; padding-bottom: 2rem; background-color: rgb(251, 239, 222); width: 100%;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">QuantiPhy Dataset</h2>
        </div>
      </div>
  <!-- Dataset Overview Subsection -->
  <div class="container is-max-desktop" style="margin-top: 1rem;">
    
    <!-- Image Slider -->
    <div class="slider-wrapper" style="margin: 1rem 0; background-color: white; padding: 1rem; border-radius: 8px;">
      <div class="slider-container" style="position: relative; overflow: hidden;">
        <div id="dataset-slider" style="display: flex; transition: transform 0.3s ease; align-items: center;">
          <div class="slide" style="min-width: 100%; box-sizing: border-box; display: flex; flex-direction: column; align-items: center; justify-content: center;">
            <img src="./static/images/data_cat.png" alt="QuantiPhy Dataset Examples" style="max-width: 90%; display: block;">
            <p class="has-text-centered" style="font-size: 0.9rem; margin-top: 0.5rem; color: rgb(0,0,0);">
              <strong>Figure 2.</strong> Sample examples from <span class="quantiphy">QuantiPhy</span>, illustrating the four core task combinations.
            </p>
    </div>
          <div class="slide" style="min-width: 100%; box-sizing: border-box; display: flex; flex-direction: column; align-items: center; justify-content: center;">
            <img src="./static/images/data_overview.png" alt="QuantiPhy Data Overview" style="max-width: 75%; display: block;">
            <p class="has-text-centered" style="font-size: 0.9rem; margin-top: 0.5rem; color: rgb(0,0,0);">
              <strong>Figure 3.</strong> Dataset statistics overview.
            </p>
  </div>
        </div>
        <!-- Navigation Arrows -->
        <button onclick="slideDataset(-1)" style="position: absolute; left: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 10px 15px; cursor: pointer; border-radius: 5px; font-size: 1.2rem;">‚ùÆ</button>
        <button onclick="slideDataset(1)" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 10px 15px; cursor: pointer; border-radius: 5px; font-size: 1.2rem;">‚ùØ</button>
        </div>
      <!-- Dots - outside overflow hidden -->
      <div style="text-align: center; margin-top: 1rem;">
        <span id="dot-0" onclick="goToSlide(0)" style="height: 12px; width: 12px; margin: 0 4px; background-color: rgb(11,61,62); border-radius: 50%; display: inline-block; cursor: pointer;"></span>
        <span id="dot-1" onclick="goToSlide(1)" style="height: 12px; width: 12px; margin: 0 4px; background-color: #bbb; border-radius: 50%; display: inline-block; cursor: pointer;"></span>
      </div>
    </div>
    
    <h1 class="title is-4" style="color: rgb(0,0,0); margin-top: 1.5rem;">Dataset Overview</h1>
    
    <script>
      let currentSlide = 0;
      const totalSlides = 2;
      
      function slideDataset(direction) {
        currentSlide += direction;
        if (currentSlide < 0) currentSlide = totalSlides - 1;
        if (currentSlide >= totalSlides) currentSlide = 0;
        updateSlider();
      }
      
      function goToSlide(index) {
        currentSlide = index;
        updateSlider();
      }
      
      function updateSlider() {
        document.getElementById('dataset-slider').style.transform = `translateX(-${currentSlide * 100}%)`;
        for (let i = 0; i < totalSlides; i++) {
          document.getElementById(`dot-${i}`).style.backgroundColor = i === currentSlide ? 'rgb(11,61,62)' : '#bbb';
        }
      }
    </script>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> introduces a rigorous benchmark for evaluating <strong>quantitative physical reasoning</strong> in Vision-Language Models. Unlike traditional VQA tasks that focus on qualitative descriptions, QuantiPhy challenges models to perform precise numerical inference grounded in physical laws.
      </p>
      
      <ul style="list-style-type: none; padding-left: 0; margin-top: 0.75rem;">
        <li style="margin-bottom: 0.4rem;">
          <strong>üÜï Novel Task: Kinematic Inference</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            We formally define a task where object size, velocity, and acceleration are treated as mutually constraining quantities.
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Input:</strong> A video clip + A <strong>single physical prior</strong> provided as text (e.g., "The car is 4 meters long" or "Gravity is 9.8 m/s¬≤").</li>
            <li><strong>Reasoning:</strong> The model must use the provided prior to recover the <strong>world-to-pixel scale</strong> and leverage kinematic equations to deduce other unknown properties of the target object.</li>
            <li><strong>Output:</strong> A precise <strong>numerical value</strong> (with units) for a target property (e.g., "The velocity at t=2s is 12.5 m/s").</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.4rem;">
          <strong>üóÇÔ∏è Structured Taxonomy</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem; margin-bottom: 0.5rem;">
            To provide fine-grained analysis of model capabilities, the benchmark is organized along two primary axes:
          </p>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Dimensionality:</strong> <strong>2D</strong> (Planar motion) vs. <strong>3D</strong> (Depth-varying motion).</li>
            <li><strong>Physical Prior:</strong> <strong>Static</strong> (Size-based) vs. <strong>Dynamic</strong> (Motion-based, e.g., Velocity/Acceleration).</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.25rem;">
          <strong>üìä Scale & Diversity</strong>
          <p class="has-text-justified" style="margin-top: 0.5rem;">
            The dataset contains <strong>3,355</strong> video-question pairs derived from <strong>569</strong> unique videos. The data spans diverse sources (Simulation, Lab, Internet) to ensure coverage across microscopic, macroscopic, and astronomical scales.
          </p>
        </li>
      </ul>
        </div>
      </div>
  
  <!-- Dataset Construction Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2rem;">
    <img src="./static/images/data_construction.png" alt="Dataset Construction Pipeline" style="max-width: 100%; display: block; margin: 0 auto; border-radius: 12px;">
    <p class="has-text-centered" style="font-size: 0.9rem; margin-top: 0.5rem; margin-bottom: 1.5rem; color: rgb(0,0,0);">
      <strong>Figure 4.</strong> QuantiPhy dataset construction pipeline.
    </p>
    <h1 class="title is-4" style="color: rgb(0,0,0);">Dataset Construction</h1>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> is built from four complementary data sources, each designed to probe quantitative physical reasoning under different visual conditions. For every video, we segment object trajectories and annotate size, velocity, and acceleration using source-appropriate, physically grounded procedures.
      </p>
      
      <ul style="list-style-type: none; padding-left: 0; margin-top: 0.75rem;">
        <li style="margin-bottom: 0.4rem;">
          <strong>üîß Simulation (Fully Controlled)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Physically grounded Blender simulations with explicitly controlled object properties and motions</li>
            <li><strong>Segmentation:</strong> Automatic object-level segmentation from the simulation engine</li>
            <li><strong>Annotation:</strong> Exact ground-truth physical quantities directly obtained from simulation parameters</li>
            <li><strong>Key advantage:</strong> Enables precise causal and counterfactual interventions.</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.4rem;">
          <strong>üß™ Laboratory (Measured Reality)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Multi-camera recordings of real objects in motion under calibrated setups</li>
            <li><strong>Segmentation:</strong> Frame-level object segmentation via semi-automatic pipelines with manual verification</li>
            <li><strong>Annotation:</strong> Physical quantities computed from measured trajectories and known object dimensions</li>
            <li><strong>Key advantage:</strong> Provides real-world grounding with reliable physical measurements.</li>
          </ul>
        </li>
        
        <li style="margin-bottom: 0.25rem;">
          <strong>üåç Internet (In-the-Wild)</strong>
          <ul style="margin-left: 1.5rem; margin-top: 0.5rem;">
            <li><strong>Collection:</strong> Curated real-world videos with visible, trackable object motion</li>
            <li><strong>Segmentation:</strong> Manual and model-assisted object segmentation under diverse visual conditions</li>
            <li><strong>Annotation:</strong> Expert annotations constrained by visual evidence and physical plausibility</li>
            <li><strong>Key advantage:</strong> Tests robustness under natural noise, clutter, and ambiguity.</li>
          </ul>
        </li>
      </ul>
        </div>
          </div>

    <!-- Figure with caption -->
    <!-- <figure style="margin: 1.5rem 0;">
      <img src="./static/images/YOUR_IMAGE.png" alt="Description" style="max-width: 100%; display: block; margin: 0 auto;">
      <figcaption style="text-align: center; margin-top: 1rem; color: rgb(0,0,0);">
        <strong>Figure X: Title.</strong>  -->
        <!-- Add your figure caption here -->
      <!-- </figcaption>
    </figure> -->
        </div>
</section>
<!-- End QuantiPhy Dataset Section -->

<!-- Evaluation on QuantiPhy Section -->
<section id="evaluation-section" class="section" style="background-color: white;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">Evaluation on QuantiPhy</h2>
      </div>
    </div>
  
  <!-- Evaluation Setup Subsection -->
  <div class="container is-max-desktop" style="margin-top: 1rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Evaluation Setup</h1>
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        <strong><span class="quantiphy">QuantiPhy</span></strong> evaluates whether vision‚Äìlanguage models can infer quantitative physical properties from visual evidence, rather than producing plausible numerical guesses. Each evaluation instance consists of a short video paired with a natural-language question, and models are required to output a single continuous numerical value, such as object size, velocity, or acceleration. All tasks are open-ended and avoid multiple-choice formats, preventing shortcut strategies based on memorization or priors.
      </p>
      <p class="has-text-justified">
        We evaluate three core tasks‚Äîsize, velocity, and acceleration estimation‚Äîusing a unified protocol across simulated, laboratory, and in-the-wild videos. Human performance is measured under the same setup, providing a reference point for visually grounded quantitative reasoning.
      </p>
      
      <!-- Explain Box: Kinematic Inference Task -->
      <div style="background: linear-gradient(135deg, rgba(11,61,62,0.08) 0%, rgba(229,76,45,0.08) 100%); border-left: 4px solid rgb(11,61,62); border-radius: 8px; padding: 1.5rem; margin-top: 1.5rem; margin-bottom: 1.5rem;">
        <h3 style="color: rgb(11,61,62); margin-bottom: 1rem; font-size: 1.1rem;">
          üß† <strong>Explain Box: Kinematic Inference Task</strong>
        </h3>
        <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
          Given a video, the model is provided with a single physical prior in world units for a source object (e.g., \(S^{\text{world}}\), \(V_t^{\text{world}}\), \(A_t^{\text{world}}\)).
        </p>
        <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
          From the corresponding pixel-space measurement, the model must infer a pixel-to-world scale \(\gamma\), such that
        </p>
        <p style="text-align: center; margin: 1rem 0;">
          \[S^{\text{world}} = \gamma S^{\text{pixel}}, \quad V_t^{\text{world}} = \gamma V_t^{\text{pixel}}, \quad A_t^{\text{world}} = \gamma A_t^{\text{pixel}}.\]
        </p>
        <p class="has-text-justified" style="color: rgb(0,0,0); margin-top: 1rem; margin-bottom: 0;">
          Using this scale, the model then quantitatively infers a target kinematic property in world coordinates.
        </p>
      </div>
      
      <p class="has-text-justified">
        To measure performance, we use <strong>Mean Relative Accuracy (MRA)</strong>, a metric designed for quantitative physical inference. For a prediction \(\hat{y}\) and ground-truth value \(y\), we compute the relative error \(|\hat{y} - y| / |y|\). A prediction is considered correct under a confidence threshold \(\theta\) if
      </p>
      <p style="text-align: center; margin: 1rem 0;">
        \[\frac{|\hat{y} - y|}{|y|} < 1 - \theta,\]
      </p>
      <p class="has-text-justified">
        where \(\theta \in \{0.5, 0.55, \ldots, 0.95\}\). The final MRA score is the average accuracy across all thresholds:
      </p>
      <p style="text-align: center; margin: 1rem 0;">
        \[\text{MRA} = \frac{1}{|C|} \sum_{\theta \in C} \mathbb{1}\left(\frac{|\hat{y} - y|}{|y|} < 1 - \theta\right),\]
      </p>
      <p class="has-text-justified">
        with \(C\) denoting the set of confidence thresholds.
      </p>
      <p class="has-text-justified">
        By rewarding approximate correctness rather than exact numerical matches, MRA provides a smoother and more informative measure of quantitative physical reasoning under real-world visual uncertainty.
          </p>
        </div>
    
    <!-- Explain Box -->
    <div style="background: linear-gradient(135deg, rgba(11,61,62,0.08) 0%, rgba(229,76,45,0.08) 100%); border-left: 4px solid rgb(11,61,62); border-radius: 8px; padding: 1.5rem; margin-top: 1.5rem;">
      <h3 style="color: rgb(11,61,62); margin-bottom: 1rem; font-size: 1.1rem;">
        üß† <strong>Explain Box: Why Mean Relative Accuracy (MRA)?</strong>
      </h3>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
        <strong>Exact numerical matching is unrealistic for physical reasoning.</strong> Small perceptual differences in video can lead to meaningful but imperfect numerical estimates, even for humans.
      </p>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 0.75rem;">
        <strong>Mean Relative Accuracy (MRA)</strong> addresses this by:
      </p>
      <ul style="margin-left: 1.5rem; color: rgb(0,0,0);">
        <li>Measuring <strong>relative error</strong> rather than absolute error</li>
        <li>Assigning <strong>partial credit</strong> for approximately correct predictions</li>
        <li>Avoiding <strong>brittle pass/fail judgments</strong> on noisy visual data</li>
      </ul>
      <p class="has-text-justified" style="color: rgb(0,0,0); margin-top: 1rem; margin-bottom: 0;">
        As a result, MRA better reflects how physical quantities are inferred in practice and provides a smoother, more informative signal for comparing models.
          </p>
        </div>
          </div>
  
  <!-- QuantiPhy Leaderboard Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2.5rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">QuantiPhy Leaderboard <img src="./static/images/QuantiPhy_logo.png" alt="" style="height: 1.2em; vertical-align: middle; margin-left: 0.3rem;"></h1>
    
    <p class="has-text-justified" style="color: rgb(0,0,0); margin-bottom: 1rem;">
      To include your model in the leaderboard, please email <a href="mailto:puyinli@stanford.edu" style="color: rgb(229,76,45);">puyinli@stanford.edu</a> with evaluation logs and setups.
    </p>
    
    <style>
      .quantiphy-leaderboard {
        width: 100%;
        border-collapse: separate;
        border-spacing: 0;
        background: #ffffff;
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        font-size: 14px;
        margin-bottom: 1rem;
      }
      .quantiphy-leaderboard thead {
        background: rgb(11,61,62);
        color: white;
      }
      .quantiphy-leaderboard th {
        padding: 12px 10px;
        text-align: center;
        font-weight: 600;
        font-size: 13px;
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        color: white;
      }
      .quantiphy-leaderboard td {
        padding: 10px 8px;
        border-bottom: 1px solid #e2e8f0;
        color: #4a5568;
        text-align: center;
      }
      .quantiphy-leaderboard tbody tr:hover {
        background-color: #f7fafc;
      }
      .quantiphy-leaderboard td:first-child {
        font-weight: 600;
        text-align: right;
        color: #2d3748;
        padding-right: 15px;
      }
      .quantiphy-leaderboard .section-header {
        background-color: rgba(11,61,62,0.1);
        font-weight: 700;
        color: #2d3748;
        font-size: 14px;
        text-align: left;
        padding: 10px 15px !important;
      }
      .quantiphy-leaderboard .section-header td {
        text-align: left;
        font-weight: 700;
        font-style: italic;
      }
      .quantiphy-leaderboard .human-row {
        background: rgba(229,76,45,0.08) !important;
        font-weight: 600;
      }
      .quantiphy-leaderboard .human-row td {
        border-top: 2px solid rgb(229,76,45);
        border-bottom: 2px solid rgb(229,76,45);
      }
      .quantiphy-leaderboard .best-score {
        font-weight: 700;
        background-color: rgba(229,76,45,0.15);
      }
      .quantiphy-leaderboard .best-open {
        font-weight: 700;
        background-color: rgba(11,61,62,0.12);
      }
      .quantiphy-leaderboard tbody tr:nth-child(even):not(.section-header):not(.human-row) {
        background-color: #fafafa;
      }
    </style>
    
    <div style="overflow-x: auto;">
      <table class="quantiphy-leaderboard">
        <thead>
          <tr>
            <th style="text-align: right;">Models</th>
            <th>Size</th>
            <th>Release</th>
            <th colspan="4" style="border-bottom: 2px solid rgba(255,255,255,0.3);">Kinematic Categories</th>
            <th>Avg.</th>
          </tr>
          <tr>
            <th></th>
            <th></th>
            <th></th>
            <th>2S</th>
            <th>2D</th>
            <th>3S</th>
            <th>3D</th>
            <th></th>
          </tr>
        </thead>
        <tbody>
          <!-- Proprietary Models -->
          <tr class="section-header">
            <td colspan="8">Proprietary models</td>
          </tr>
          <tr>
            <td>ChatGPT-5.1</td>
            <td>‚Äì</td>
            <td>2025/11</td>
            <td class="best-score">46.1</td>
            <td>56.1</td>
            <td class="best-score">45.0</td>
            <td class="best-score">58.1</td>
            <td class="best-score">52.6</td>
          </tr>
          <tr>
            <td>Gemini-2.5 Pro</td>
            <td>‚Äì</td>
            <td>2025/03</td>
            <td>44.3</td>
            <td class="best-score">57.3</td>
            <td>41.8</td>
            <td>53.6</td>
            <td>49.3</td>
          </tr>
          <tr>
            <td>Gemini-2.5 Flash</td>
            <td>‚Äì</td>
            <td>2025/03</td>
            <td>39.9</td>
            <td>53.0</td>
            <td>40.6</td>
            <td>56.2</td>
            <td>47.4</td>
          </tr>
          <tr>
            <td>ChatGPT-5</td>
            <td>‚Äì</td>
            <td>2025/04</td>
            <td>36.4</td>
            <td>34.9</td>
            <td>24.7</td>
            <td>32.9</td>
            <td>32.2</td>
          </tr>
          <tr>
            <td>Claude-4.5 Sonnet</td>
            <td>‚Äì</td>
            <td>2025/04</td>
            <td>19.6</td>
            <td>22.9</td>
            <td>19.6</td>
            <td>29.1</td>
            <td>22.8</td>
          </tr>
          
          <!-- Open-weight Models -->
          <tr class="section-header">
            <td colspan="8">Open-weight models</td>
          </tr>
          <tr>
            <td>InternVL-3.5-30B</td>
            <td>30B</td>
            <td>2025/05</td>
            <td class="best-open">36.7</td>
            <td>45.4</td>
            <td class="best-open">38.0</td>
            <td class="best-open">41.6</td>
            <td class="best-open">40.4</td>
          </tr>
          <tr>
            <td>Qwen3-VL-Instruct-8B</td>
            <td>8B</td>
            <td>2025/04</td>
            <td>26.0</td>
            <td class="best-open">47.8</td>
            <td>33.5</td>
            <td>46.0</td>
            <td>38.3</td>
          </tr>
          <tr>
            <td>Molmo-7B</td>
            <td>7B</td>
            <td>2024/09</td>
            <td>30.0</td>
            <td>43.1</td>
            <td>24.4</td>
            <td>36.6</td>
            <td>33.5</td>
          </tr>
          <tr>
            <td>Phi-4-Multimodal-Instruct</td>
            <td>5.6B</td>
            <td>2025/02</td>
            <td>33.3</td>
            <td>42.3</td>
            <td>23.2</td>
            <td>27.9</td>
            <td>31.7</td>
          </tr>
          <tr>
            <td>SmolVLM-Instruct</td>
            <td>0.26B</td>
            <td>2024/11</td>
            <td>31.6</td>
            <td>34.4</td>
            <td>19.8</td>
            <td>27.8</td>
            <td>28.4</td>
          </tr>
          <tr>
            <td>VILA-7B</td>
            <td>7B</td>
            <td>2024/01</td>
            <td>23.0</td>
            <td>29.8</td>
            <td>14.4</td>
            <td>23.0</td>
            <td>22.6</td>
          </tr>
          <tr>
            <td>CogVLM2 Video</td>
            <td>12B</td>
            <td>2024/05</td>
            <td>19.4</td>
            <td>28.7</td>
            <td>12.7</td>
            <td>27.9</td>
            <td>22.2</td>
          </tr>
          <tr>
            <td>Phi-3-Mini-128K-Instruct-3.8B</td>
            <td>3.8B</td>
            <td>2024/04</td>
            <td>17.3</td>
            <td>14.7</td>
            <td>19.2</td>
            <td>18.5</td>
            <td>17.4</td>
          </tr>
          <tr>
            <td>LLaVA-13B</td>
            <td>13B</td>
            <td>2023/10</td>
            <td>14.4</td>
            <td>22.1</td>
            <td>8.0</td>
            <td>16.5</td>
            <td>15.2</td>
          </tr>
          <tr>
            <td>MiniCPM-V 4.5-8B</td>
            <td>8B</td>
            <td>2025/05</td>
            <td>27.6</td>
            <td>26.1</td>
            <td>0.4</td>
            <td>0.0</td>
            <td>13.5</td>
          </tr>
          <tr>
            <td>Fuyu-8B</td>
            <td>8B</td>
            <td>2023/10</td>
            <td>9.5</td>
            <td>14.7</td>
            <td>9.5</td>
            <td>16.2</td>
            <td>12.5</td>
          </tr>
          
          <!-- Human Baseline -->
          <tr class="human-row">
            <td>Human Baseline</td>
            <td>‚Äì</td>
            <td>‚Äì</td>
            <td>50.0</td>
            <td>59.1</td>
            <td>55.2</td>
            <td>57.9</td>
            <td>55.6</td>
          </tr>
        </tbody>
      </table>
            </div>
    
    <p style="font-size: 0.85rem; color: #666; margin-top: 0.5rem;">
      <strong>Note:</strong> Best proprietary model scores are highlighted in <span style="background-color: rgba(229,76,45,0.15); padding: 2px 6px; border-radius: 3px;">orange</span>, best open-weight model scores in <span style="background-color: rgba(11,61,62,0.12); padding: 2px 6px; border-radius: 3px;">teal</span>.
    </p>
          </div>
</section>
<!-- End Evaluation on QuantiPhy Section -->

<!-- Dissecting Quantitative Reasoning Section -->
<section id="dissecting-section" style="padding-top: 1rem; padding-bottom: 2rem; background-color: rgb(251, 239, 222); width: 100%;">
  <div style="width: 100%; padding: 0;">
    <div class="has-text-centered">
      <h2 class="title is-3" style="color: rgb(0,0,0);"><img src="./static/images/QuantiPhy_logo_pure.png" alt="QuantiPhy Logo" style="height: 1.5em; vertical-align: middle; margin-right: 0.5rem;">Dissecting Quantitative Reasoning in Vision‚ÄìLanguage Models</h2>
          </div>
        </div>
  
  <div class="container" style="margin-top: 1rem; max-width: 1200px; padding: 0 2rem;">
    
    <!-- Case Study Slider -->
    <div class="case-slider-wrapper" style="margin: 1rem 0; background-color: white; padding: 1.5rem 2rem; border-radius: 12px; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
      
      <!-- Slider Title -->
      <div style="text-align: center; margin-bottom: 1.5rem; padding-bottom: 1rem; border-bottom: 2px solid rgba(11,61,62,0.15);">
        <h3 style="color: rgb(11,61,62); font-size: 1.4rem; font-weight: 700; margin: 0;">
          How Does GPT-5.1 Approach Kinematic Inference Tasks?
        </h3>
      </div>
      
      <div class="case-slider-container" style="position: relative; overflow: hidden;">
        <div id="case-slider" style="display: flex; transition: transform 0.4s ease;">
          
          <!-- Case 1 -->
          <div class="case-slide" style="min-width: 100%; box-sizing: border-box; display: flex; justify-content: center; padding: 0;">
            <div style="display: flex; gap: 2rem; align-items: center; max-width: 960px; width: 100%;">
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/case1.png" alt="Case 1" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
              </div>
              <div style="flex: 1; padding: 0.5rem 0;">
                <div style="background: linear-gradient(135deg, rgb(11,61,62) 0%, rgb(20,90,92) 100%); color: white; padding: 0.5rem 1rem; border-radius: 6px; display: inline-block; margin-bottom: 1rem;">
                  <strong style="font-size: 1.1rem; color: white;">Case 1 ‚Äî When Visual Measurement Works</strong>
                </div>
                <p style="color: rgb(0,0,0); line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.95rem;">
                  <strong style="color: rgb(11,61,62);">GPT-5.1 can infer kinematic quantities when visual cues are clean and well-scaled.</strong>
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  Given a realistic object size and clear motion across frames, the model explicitly estimates pixel displacement, maps it to real-world units, and computes speed or dimensions proportionally.
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  In these cases, predictions closely match ground truth, suggesting genuine use of visual evidence rather than memorized priors.
                </p>
                <p style="color: rgb(229,76,45); line-height: 1.7; font-size: 0.9rem; font-style: italic; border-left: 3px solid rgb(229,76,45); padding-left: 0.8rem;">
                  This represents GPT-5.1's best-case behavior: explicit visual measurement followed by numerical reasoning.
                </p>
              </div>
            </div>
          </div>

          <!-- Case 2 -->
          <div class="case-slide" style="min-width: 100%; box-sizing: border-box; display: flex; justify-content: center; padding: 0;">
            <div style="display: flex; gap: 2rem; align-items: center; max-width: 960px; width: 100%;">
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/case2.png" alt="Case 2" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
              </div>
              <div style="flex: 1; padding: 0.5rem 0;">
                <div style="background: linear-gradient(135deg, rgb(11,61,62) 0%, rgb(20,90,92) 100%); color: white; padding: 0.5rem 1rem; border-radius: 6px; display: inline-block; margin-bottom: 1rem;">
                  <strong style="font-size: 1.1rem; color: white;">Case 2 ‚Äî Sensitivity to Unrealistic Scale</strong>
                </div>
                <p style="color: rgb(0,0,0); line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.95rem;">
                  <strong style="color: rgb(11,61,62);">Unrealistic physical scales can derail otherwise correct visual reasoning.</strong>
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  When object dimensions are inflated to implausible values (e.g., a car length of 5670 m), GPT-5.1 often performs arithmetic consistently but fails to detect scale violations.
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  The model propagates the incorrect assumption through its calculations, leading to extreme numerical errors in downstream kinematic estimates.
                </p>
                <p style="color: rgb(229,76,45); line-height: 1.7; font-size: 0.9rem; font-style: italic; border-left: 3px solid rgb(229,76,45); padding-left: 0.8rem;">
                  Visual reasoning remains internally consistent, but lacks physical plausibility checks.
                </p>
              </div>
            </div>
          </div>
          
          <!-- Case 3 -->
          <div class="case-slide" style="min-width: 100%; box-sizing: border-box; display: flex; justify-content: center; padding: 0;">
            <div style="display: flex; gap: 2rem; align-items: center; max-width: 960px; width: 100%;">
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/case3.png" alt="Case 3" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
              </div>
              <div style="flex: 1; padding: 0.5rem 0;">
                <div style="background: linear-gradient(135deg, rgb(11,61,62) 0%, rgb(20,90,92) 100%); color: white; padding: 0.5rem 1rem; border-radius: 6px; display: inline-block; margin-bottom: 1rem;">
                  <strong style="font-size: 1.1rem; color: white;">Case 3 ‚Äî Guessing Without Visual Evidence</strong>
                </div>
                <p style="color: rgb(0,0,0); line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.95rem;">
                  <strong style="color: rgb(11,61,62);">When visual input is removed, GPT-5.1 falls back to prior-driven guesses.</strong>
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  With the video ablated, the model no longer has access to motion cues and instead produces values based on typical real-world expectations (e.g., "reasonable" speeds or object widths).
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  These guesses often deviate substantially from ground truth and show little task-specific adaptation.
                </p>
                <p style="color: rgb(229,76,45); line-height: 1.7; font-size: 0.9rem; font-style: italic; border-left: 3px solid rgb(229,76,45); padding-left: 0.8rem;">
                  Without visual grounding, numerical outputs reflect priors rather than inference.
                </p>
              </div>
            </div>
          </div>
          
          <!-- Case 4 -->
          <div class="case-slide" style="min-width: 100%; box-sizing: border-box; display: flex; justify-content: center; padding: 0;">
            <div style="display: flex; gap: 2rem; align-items: center; max-width: 960px; width: 100%;">
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/case4.png" alt="Case 4" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
              </div>
              <div style="flex: 1; padding: 0.5rem 0;">
                <div style="background: linear-gradient(135deg, rgb(11,61,62) 0%, rgb(20,90,92) 100%); color: white; padding: 0.5rem 1rem; border-radius: 6px; display: inline-block; margin-bottom: 1rem;">
                  <strong style="font-size: 1.1rem; color: white;">Case 4 ‚Äî Prior Dominance</strong>
                </div>
                <p style="color: rgb(0,0,0); line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.95rem;">
                  <strong style="color: rgb(11,61,62);">For acceleration-related tasks, prior knowledge can override visual evidence.</strong>
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  In scenarios involving falling objects, GPT-5.1 frequently outputs values close to canonical constants (e.g., gravitational acceleration), even when the video implies different kinematic behavior.
                </p>
                <p style="color: #444; line-height: 1.7; margin-bottom: 0.8rem; font-size: 0.9rem;">
                  This pattern suggests reliance on memorized physics facts instead of extracting acceleration from observed motion.
                </p>
                <p style="color: rgb(229,76,45); line-height: 1.7; font-size: 0.9rem; font-style: italic; border-left: 3px solid rgb(229,76,45); padding-left: 0.8rem;">
                  Correct-looking numbers do not necessarily imply visually grounded reasoning.
                </p>
              </div>
            </div>
          </div>
          
    </div>
        <!-- Navigation Arrows -->
        <button onclick="slideCases(-1)" style="position: absolute; left: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 12px 16px; cursor: pointer; border-radius: 5px; font-size: 1.2rem; z-index: 10;">‚ùÆ</button>
        <button onclick="slideCases(1)" style="position: absolute; right: 0; top: 50%; transform: translateY(-50%); background: rgba(11,61,62,0.85); color: white; border: none; padding: 12px 16px; cursor: pointer; border-radius: 5px; font-size: 1.2rem; z-index: 10;">‚ùØ</button>
      </div>
      <!-- Dots -->
      <div style="text-align: center; margin-top: 1.5rem;">
        <span id="case-dot-0" onclick="goToCaseSlide(0)" style="height: 12px; width: 12px; margin: 0 6px; background-color: rgb(11,61,62); border-radius: 50%; display: inline-block; cursor: pointer; transition: background-color 0.3s;"></span>
        <span id="case-dot-1" onclick="goToCaseSlide(1)" style="height: 12px; width: 12px; margin: 0 6px; background-color: #bbb; border-radius: 50%; display: inline-block; cursor: pointer; transition: background-color 0.3s;"></span>
        <span id="case-dot-2" onclick="goToCaseSlide(2)" style="height: 12px; width: 12px; margin: 0 6px; background-color: #bbb; border-radius: 50%; display: inline-block; cursor: pointer; transition: background-color 0.3s;"></span>
        <span id="case-dot-3" onclick="goToCaseSlide(3)" style="height: 12px; width: 12px; margin: 0 6px; background-color: #bbb; border-radius: 50%; display: inline-block; cursor: pointer; transition: background-color 0.3s;"></span>
      </div>
    </div>
    
    <script>
      let currentCaseSlide = 0;
      const totalCaseSlides = 4;
      
      function slideCases(direction) {
        currentCaseSlide += direction;
        if (currentCaseSlide < 0) currentCaseSlide = totalCaseSlides - 1;
        if (currentCaseSlide >= totalCaseSlides) currentCaseSlide = 0;
        updateCaseSlider();
      }
      
      function goToCaseSlide(index) {
        currentCaseSlide = index;
        updateCaseSlider();
      }
      
      function updateCaseSlider() {
        document.getElementById('case-slider').style.transform = `translateX(-${currentCaseSlide * 100}%)`;
        for (let i = 0; i < totalCaseSlides; i++) {
          document.getElementById(`case-dot-${i}`).style.backgroundColor = i === currentCaseSlide ? 'rgb(11,61,62)' : '#bbb';
        }
      }
    </script>

  </div>
  
  <!-- Scene Context Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Scene Context and Relational Cues</h1>
    
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified">
        We first examine how <strong>scene context</strong> affects quantitative reasoning by varying background complexity and the number of moving objects. Surprisingly, background complexity alone has only a mild effect on performance. While removing clutter via segmentation slightly stabilizes predictions, models often perform equally well‚Äîor even better‚Äîin visually rich scenes, likely due to the presence of implicit reference cues such as tiles, road markings, or architectural structures.
      </p>
      
      <p class="has-text-justified">
        In contrast, the <strong>number of objects</strong> in a scene has a consistent and substantial impact. Across models, multi-object scenes outperform single-object setups, suggesting that relational structure provides valuable comparative anchors for inferring both size and motion.
      </p>
      
      <!-- Images side by side -->
      <div style="display: flex; gap: 1.5rem; margin: 1.5rem 0; flex-wrap: wrap; justify-content: center;">
        <div style="flex: 1; min-width: 300px; max-width: 55%;">
          <img src="./static/images/mra_line_plot.png" alt="MRA Line Plot - Background Complexity and Object Count" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
          <p class="has-text-centered" style="font-size: 0.85rem; margin-top: 0.5rem; color: #666;">
            <strong>Figure 5.</strong> Performance across background complexity and object count conditions.
          </p>
        </div>
        <div style="flex: 1; min-width: 300px; max-width: 40%;">
          <img src="./static/images/s_m.png" alt="Single vs Multiple Objects" style="width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
          <p class="has-text-centered" style="font-size: 0.85rem; margin-top: 0.5rem; color: #666;">
            <strong>Figure 6.</strong> Video type classification: single (S) vs. multiple (M) objects.
          </p>
        </div>
      </div>
      
      <!-- Takeaway Box -->
      <div style="background: linear-gradient(135deg, rgba(11,61,62,0.08) 0%, rgba(229,76,45,0.08) 100%); border-left: 4px solid rgb(11,61,62); border-radius: 8px; padding: 1.2rem 1.5rem; margin-top: 1.5rem;">
        <p style="margin: 0; color: rgb(0,0,0);">
          <strong style="color: rgb(11,61,62);">üí° Takeaway:</strong> VLMs benefit less from visual simplicity than from relational and contextual cues embedded in the scene.
          </p>
        </div>
      </div>
    </div>
  
  <!-- Input Faithfulness Subsection -->
  <div class="container is-max-desktop" style="margin-top: 2.5rem;">
    <h1 class="title is-4" style="color: rgb(0,0,0);">Input Faithfulness: Video, Priors, and Prompting</h1>
    
    <div class="content" style="color: rgb(0,0,0);">
      <p class="has-text-justified" style="font-style: italic; color: #555; margin-bottom: 1rem;">
        "Do VLMs actually use what we give them?"
      </p>
      
      <p class="has-text-justified">
        We next investigate whether VLMs faithfully condition on the reference video and the explicit physical prior provided in the prompt. We evaluate models under three controlled settings: the default <strong>video + prior</strong> condition, a <strong>prior-only</strong> condition with the video removed, and a <strong>counterfactual prior</strong> condition where the numerical prior is systematically rescaled.
      </p>
      
      <p class="has-text-justified">
        If models relied on visual measurement and algebraic reasoning, removing the video or altering the prior should lead to substantial performance degradation. Instead, we observe a striking pattern: many models achieve comparable accuracy even without the video, indicating that plausible guesses based on memorized world knowledge already suffice. When the prior is counterfactually rescaled, performance collapses across nearly all models, with predictions remaining anchored to typical real-world magnitudes rather than the provided numerical values.
      </p>
      
      <p class="has-text-justified">
        Finally, we test whether structured <strong>chain-of-thought (CoT) prompting</strong> can mitigate these failures. Contrary to expectation, step-by-step decomposition does not systematically improve performance. For most models, explicitly reasoning through pixel measurement and scale estimation amplifies early numerical errors rather than correcting them.
      </p>
      
      <!-- Input Faithfulness Table -->
      <div style="overflow-x: auto; margin: 1.5rem 0;">
        <table style="width: 100%; border-collapse: collapse; background: #ffffff; border-radius: 8px; overflow: hidden; box-shadow: 0 2px 8px rgba(0,0,0,0.08); font-size: 14px;">
          <thead>
            <tr style="background: rgb(11,61,62);">
              <th style="padding: 12px 15px; text-align: left; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">Models</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">Size</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">Release</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); border-left: 2px solid rgba(255,255,255,0.3); color: white;">Video + Prior</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">Prior only</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">Counterfactual</th>
              <th style="padding: 12px 10px; text-align: center; font-weight: 600; border-bottom: 1px solid rgba(255,255,255,0.1); color: white;">CoT</th>
            </tr>
          </thead>
          <tbody>
            <!-- Proprietary Models -->
            <tr style="background-color: rgba(11,61,62,0.08);">
              <td colspan="7" style="padding: 10px 15px; font-weight: 700; font-style: italic; color: #2d3748;">Proprietary models</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">ChatGPT-5.1</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/11</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">56.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">45.7</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">15.4</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">26.5</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Gemini-2.5 Pro</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/03</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">59.2</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">45.0</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">29.9</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">48.7</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Gemini-2.5 Flash</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/03</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">49.2</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">35.9</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">13.8</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">21.9</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">ChatGPT-5</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/04</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">33.5</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">50.2</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">29.6</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">53.0</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Claude-4.5 Sonnet</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/04</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">25.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">15.7</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">11.4</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">25.9</td>
            </tr>
            
            <!-- Open-weight Models -->
            <tr style="background-color: rgba(11,61,62,0.08);">
              <td colspan="7" style="padding: 10px 15px; font-weight: 700; font-style: italic; color: #2d3748;">Open-weight models</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">InternVL-3.5-30B</td>
              <td style="padding: 10px; text-align: center; color: #666;">30B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/05</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">45.4</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">12.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">17.6</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Qwen3-VL-Instruct-8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/04</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">40.5</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">24.9</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">12.0</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">21.0</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Molmo-7B</td>
              <td style="padding: 10px; text-align: center; color: #666;">7B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2024/09</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">39.8</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">14.7</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">15.9</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Phi-4-Multimodal-Instruct</td>
              <td style="padding: 10px; text-align: center; color: #666;">5.6B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/02</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">40.0</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">20.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">9.2</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">11.6</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">SmolVLM-Instruct</td>
              <td style="padding: 10px; text-align: center; color: #666;">0.26B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2024/11</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">38.9</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">14.3</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">17.8</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">VILA-7B</td>
              <td style="padding: 10px; text-align: center; color: #666;">7B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2024/01</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">31.8</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">14.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">10.0</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">CogVLM2 Video</td>
              <td style="padding: 10px; text-align: center; color: #666;">12B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2024/05</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">28.5</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">9.5</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">26.4</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Phi-3-Mini-128K-Instruct-3.8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">3.8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2024/04</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">11.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">10.3</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">8.2</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">7.2</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">LLaVA-13B</td>
              <td style="padding: 10px; text-align: center; color: #666;">13B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2023/10</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">20.2</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">13.9</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">14.4</td>
            </tr>
            <tr style="background: #fafafa;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">MiniCPM-V 4.5-8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2025/05</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">29.3</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">19.1</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">24.1</td>
            </tr>
            <tr style="background: #fff;">
              <td style="padding: 10px 15px; font-weight: 500; color: #2d3748; text-align: right;">Fuyu-8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">8B</td>
              <td style="padding: 10px; text-align: center; color: #666;">2023/10</td>
              <td style="padding: 10px; text-align: center; color: #2d3748; border-left: 2px solid #e2e8f0;">14.3</td>
              <td style="padding: 10px; text-align: center; color: #666;">‚Äì</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">9.0</td>
              <td style="padding: 10px; text-align: center; color: #2d3748;">21.1</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p class="has-text-centered" style="font-size: 0.85rem; color: #666; margin-top: 0.5rem;">
        <strong>Table 2.</strong> Input faithfulness analysis across different experimental conditions.
      </p>
      
      <!-- Takeaway Box -->
      <div style="background: linear-gradient(135deg, rgba(11,61,62,0.08) 0%, rgba(229,76,45,0.08) 100%); border-left: 4px solid rgb(11,61,62); border-radius: 8px; padding: 1.2rem 1.5rem; margin-top: 1.5rem;">
        <p style="margin: 0; color: rgb(0,0,0);">
          <strong style="color: rgb(11,61,62);">üí° Takeaway:</strong> Current VLMs are not input-faithful quantitative reasoners: visual evidence and explicit priors act as soft hints, while memorized world knowledge dominates final predictions.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End Dissecting Quantitative Reasoning Section -->

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{li2026quantiphy,
  author    = {Li, Puyin and Xiang, Tiange and Mao, Ella and Wei, Shirley and Chen, Xinye and Masood, Adnan and Li, Fei-Fei and Adeli, Ehsan},
  title     = {QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models},
  journal   = {},
  year      = {2026},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>

